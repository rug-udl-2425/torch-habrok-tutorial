{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training a NN\n",
    "\n",
    "In this part of the tutrorial, we'll be training a simple MLP on MNIST.\n",
    "We'll go over the basic components for the training part:\n",
    "- Datasets\n",
    "- DataLoaders\n",
    "- The basic training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# tqdm adds support for progress bars\n",
    "from tqdm import tqdm\n",
    "\n",
    "# torchvision provides support for computer vision (datasets, transformations, models,...)\n",
    "import torchvision\n",
    "from torchvision import transforms as T\n",
    "\n",
    "# torcheval provides support for evaluation metrics\n",
    "from torcheval import metrics\n",
    "\n",
    "# timm is a HuggingFace library providing a large collection of pre-trained image models (>> torchvision)\n",
    "import timm\n",
    "\n",
    "# extra libraries\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll first download the MNIST dataset and then define the model and the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = torchvision.datasets.MNIST(root=\"data\", train=True, download=True)\n",
    "mnist_test = torchvision.datasets.MNIST(root=\"data\", train=False, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets to be used in PyTorch should be subclasses of `torch.utils.data.Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(mnist_train, torch.utils.data.Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Dataset` provides the basic interface to access the data and defines data augmentation/preprocessing steps:\n",
    "- the `transform` attribute provides the information about the preprocessing steps to be applied to the data\n",
    "- `__len__` should return the size of the dataset\n",
    "- `__getitem__` should return the item at the given index **after applying preprocessing**\n",
    "\n",
    "![](img/dataset.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train.transform is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, the dataset essentially contains basic images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = mnist_train[0]\n",
    "img = img.copy()\n",
    "\n",
    "img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add some basic transformations to the dataset so it can be elaborated by the dataset:\n",
    "\n",
    "- `ToTensor` converts the image to a tensor and normalizes it in the 0-1 range. This is a **preprocessing step** since it's not deterministic. \n",
    "  - We can very well convert the whole dataset to tensors beforehand. **Question**: What prevents us from doing this?\n",
    "- `RandomAffine` applies a random affine transformation (rotation + translation + scaling) to the image. This is a **data augmentation step** since it's non-deterministic and we want to apply it on the fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.RandomAffine(degrees=10, scale=(0.9, 1.1)),\n",
    "])\n",
    "\n",
    "mnist_train.transform = transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img2, label = mnist_train[0]\n",
    "\n",
    "# plot 2 images side by side\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axs[0].imshow(img, cmap=\"gray\")\n",
    "axs[1].imshow((img2.permute(1, 2, 0) * 255).numpy().astype(\"uint8\"), cmap=\"gray\")\n",
    "plt.title(\"Transformed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to add the `ToTensor` transformation to the `test_transform` since we want to convert the images to tensors before feeding them to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_test.transform = T.ToTensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a model using SGD, we need to pack the data into batches. This is done using `DataLoader` which is a subclass of `torch.utils.data.DataLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(mnist_train, batch_size=64, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(mnist_test, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A DataLoader implements the iterator pattern in a **lazy way** (using a generator):\n",
    "- We cannot access the batches directly (e.g., `trainloader[0]` does not work)\n",
    "- We create a dataset by iterating over the DataLoader or by calling the `next` method on `iter(dataloader)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(trainloader):\n",
    "    print(f\"Batch {i+1}\", batch[0].shape, batch[1].shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define the model. We'll be using a simple MLP with 3 hidden layers, relu activations, and batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(28*28, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(16),\n",
    "    nn.Linear(16, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(16),\n",
    "    nn.Linear(16, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(16),\n",
    "    nn.Linear(16, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, our model needs an **optimizer** to update the weights. We'll use the `torch.optim.SGD` optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(mlp.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now build the training loop.\n",
    "\n",
    "We define a number of epochs for training. In each epoch, we'll *regenerate* the trainloader to redo the batches and apply the random transformations to the images.\n",
    "\n",
    "```python\n",
    "for epoch in range(epochs):\n",
    "    for data, labels in trainloader:\n",
    "        ...\n",
    "```\n",
    "\n",
    "Within the inner loop, we will need to do the following things:\n",
    "- Zero out the gradients from the previous iteration\n",
    "- Do the forward pass\n",
    "- Compute the loss\n",
    "- Do the backward pass\n",
    "- Update the weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.train() # IMPORTANT: this sets the model to training mode --- useful for batchnorm and dropout\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for data, labels in trainloader:\n",
    "        # inner loop\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often useful to compute some other metrics while the model is training.\n",
    "\n",
    "We can use `torcheval` functionalities to keep track of accuracy and loss.\n",
    "\n",
    "In addition, we can use `tqdm` to display a progress bar.\n",
    "\n",
    "We will also anneal the learning rate at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.train() # IMPORTANT: this sets the model to training mode --- useful for batchnorm and dropout\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1) # will anneal LR by 0.1 each time scheduler.step() is called\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    accuracy_counter = metrics.MulticlassAccuracy()\n",
    "    loss_counter = metrics.Mean()\n",
    "\n",
    "    progress_bar = tqdm(trainloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for data, labels in progress_bar:\n",
    "        # some code here\n",
    "\n",
    "        accuracy_counter.update(predictions, labels)\n",
    "        loss_counter.update(loss, weight=data.size(0))\n",
    "\n",
    "        progress_bar.set_postfix(\n",
    "            loss=loss_counter.compute().item(),\n",
    "            accuracy=accuracy_counter.compute().item()\n",
    "        )\n",
    "    scheduler.step() # anneal LR by 0.1\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test evaluation is done in a similar way to the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.eval() # IMPORTANT: this sets the model to evaluation mode --- useful for batchnorm and dropout\n",
    "\n",
    "accuracy_counter = metrics.MulticlassAccuracy()\n",
    "loss_counter = metrics.Mean()\n",
    "\n",
    "progress_bar = tqdm(testloader, desc=f\"Eval 1/1\")\n",
    "for data, labels in progress_bar:\n",
    "    with torch.no_grad(): # force no gradients\n",
    "        # some code here\n",
    "\n",
    "\n",
    "        accuracy_counter.update(predictions, labels)\n",
    "        loss_counter.update(loss, weight=data.size(0))\n",
    "\n",
    "        progress_bar.set_postfix(\n",
    "            loss=loss_counter.compute().item(),\n",
    "            accuracy=accuracy_counter.compute().item()\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's bundle together all the components to train and eval the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, scheduler, trainloader, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        accuracy_counter = metrics.MulticlassAccuracy()\n",
    "        loss_counter = metrics.Mean()\n",
    "\n",
    "        progress_bar = tqdm(trainloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for data, labels in progress_bar:\n",
    "\n",
    "            accuracy_counter.update(predictions, labels)\n",
    "            loss_counter.update(loss, weight=data.size(0))\n",
    "\n",
    "            progress_bar.set_postfix(\n",
    "                loss=loss_counter.compute().item(),\n",
    "                accuracy=accuracy_counter.compute().item()\n",
    "            )\n",
    "        scheduler.step()\n",
    "\n",
    "def evaluate(model, testloader):\n",
    "    model.eval()\n",
    "    accuracy_counter = metrics.MulticlassAccuracy()\n",
    "    loss_counter = metrics.Mean()\n",
    "\n",
    "    progress_bar = tqdm(testloader, desc=\"Eval 1/1\")\n",
    "    for data, labels in progress_bar:\n",
    "        with torch.no_grad():\n",
    "            \n",
    "\n",
    "            progress_bar.set_postfix(\n",
    "                loss=loss_counter.compute().item(),\n",
    "                accuracy=accuracy_counter.compute().item()\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and loading a model\n",
    "\n",
    "We can save the model using `torch.save(model.state_dict(), save_path)` and load it using `model.load_state_dict(torch.load(save_path))`.\n",
    "\n",
    "The `state_dict` is a dictionary containing the model's parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"weights/mlp_mnist.pth\"\n",
    "\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "torch.save(mlp.state_dict(), \"weights/mlp_mnist.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.load_state_dict(torch.load(\"weights/mlp_mnist.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice: also optimizers, schedulers, and metrics have a state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_counter.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving checkpoints\n",
    "\n",
    "You can also save checkpoints during training. This is useful in case the training is interrupted and you want to resume from the last checkpoint.\n",
    "\n",
    "The checkpoint should contain all the information needed to resume training:\n",
    "- The epoch number\n",
    "  - In case you're saving the checkpoint mid-epoch, you must save the iteration number as well, and also the partial metrics\n",
    "- The model's state_dict\n",
    "- The optimizer's state_dict\n",
    "- If you're using a learning rate scheduler, you should save its state_dict as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer learning\n",
    "\n",
    "We can do transfer learning by loading a pretrained model and changing the last layer to match the number of classes in our dataset.\n",
    "\n",
    "`timm` and `torchvision` provide a number of pretrained models that can be used for transfer learning.\n",
    "\n",
    "We can create the models by passing the `pretrained=True` argument and the num_classes argument to match the number of classes in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pretrained = timm.create_model(\"resnet18\", pretrained=True, num_classes=10)\n",
    "\n",
    "model_pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scratch = timm.create_model(\"resnet18\", pretrained=False, num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We additionaly need to modify our dataset since ResNet18 is a CNN and expects a 3-channel input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train.transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.RandomAffine(degrees=10, scale=(0.9, 1.1)),\n",
    "    T.Lambda(lambda x: x.repeat(3, 1, 1)), # repeat the single channel 3 times to get 3 channels\n",
    "    \n",
    "])\n",
    "\n",
    "mnist_test.transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "\n",
    "optimizer_pretrained = torch.optim.SGD(model_pretrained.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0001)\n",
    "scheduler_pretrained = torch.optim.lr_scheduler.StepLR(optimizer_pretrained, step_size=1, gamma=0.1)\n",
    "\n",
    "train(model_pretrained, optimizer_pretrained, scheduler_pretrained, trainloader, num_epochs)\n",
    "\n",
    "optimizer_scratch = torch.optim.SGD(model_scratch.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0001)\n",
    "scheduler_scratch = torch.optim.lr_scheduler.StepLR(optimizer_scratch, step_size=1, gamma=0.1)\n",
    "\n",
    "train(model_scratch, optimizer_scratch, scheduler_scratch, trainloader, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can additionally freeze the hidden layers of the pretrained model by setting `requires_grad=False` for the parameters of the hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model_pretrained.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the last layer\n",
    "for param in model_pretrained.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# you can proceed to retrain the model. Note: you have to reload the parameters!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using acceleration\n",
    "\n",
    "We can use the GPU to accelerate the training process.\n",
    "\n",
    "We can move the model and the data to the GPU and CPU using the `to` method.\n",
    "\n",
    "`to` accepts a device argument which can be either `cuda` or `cpu`.\n",
    "\n",
    "In case of multiple GPUs available, we can force the usage of a specific GPU by specifying the device number (e.g., `cuda:0` for first GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    x = x.to(\"cuda\")\n",
    "else:\n",
    "    raise RuntimeError(\"CUDA not available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training/evaluating a model on CUDA, we need to shift to the GPU the following:\n",
    "- The model (`model.to(device)`)\n",
    "- The data (`data = data.to(device)`)\n",
    "- The labels (`labels = labels.to(device)`)\n",
    "\n",
    "In addition, for single-node multi-GPU training, we can use `torch.nn.DataParallel` to parallelize the training process.\n",
    "\n",
    "We can wrap the model using `torch.nn.DataParallel(model)` and PyTorch will automatically distribute the batches to the GPUs.\n",
    "\n",
    "```python\n",
    "\n",
    "model = nn.DataParallel(model)\n",
    "```\n",
    "\n",
    "#### Using mixed precision\n",
    "\n",
    "We can use mixed precision (`FLOAT16/FLOAT32`) training to speed up the training process.\n",
    "\n",
    "We can use the `torch.cuda.amp` module to enable mixed precision training.\n",
    "\n",
    "We can use the `torch.cuda.amp.autocast` context manager to automatically cast the inputs to the model to `FLOAT16`.\n",
    "\n",
    "We can use the `torch.cuda.amp.GradScaler` to scale the loss to avoid underflow/overflow issues.\n",
    "\n",
    "```python\n",
    "\n",
    "def train_mixed_precision(model, optimizer, scheduler, trainloader, epochs, device):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        accuracy_counter = metrics.MulticlassAccuracy().to(device)\n",
    "        loss_counter = metrics.Mean().to(device)\n",
    "\n",
    "        progress_bar = tqdm(trainloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for data, labels in progress_bar:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                predictions = model(data)\n",
    "                loss = nn.functional.cross_entropy(predictions, labels)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            accuracy_counter.update(predictions, labels)\n",
    "            loss_counter.update(loss, weight=data.size(0))\n",
    "\n",
    "            progress_bar.set_postfix(\n",
    "                loss=loss_counter.compute().item(),\n",
    "                accuracy=accuracy_counter.compute().item()\n",
    "            )\n",
    "        scheduler.step()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
