{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PyTorch modules and automatic differentiation\n",
    "\n",
    "PyTorch provides the users with an extensive set of modules and functions for machine learning.\n",
    "The basis of the modules is the `torch.nn.Module` base class, which provides a basic blueprint for a functioning of a neural network layer.\n",
    "The basic idea is that each component of the model should be a subclass of `Module` (including the model itself).\n",
    "With the inheritance from `Module` come attributes and method which allow PyTorch to seamlessly handle the model and its parameters.\n",
    "\n",
    "For instance, the method `parameters()` returns an iterator over the parameters of the model, which can be used to update the parameters during the training.\n",
    "It is one of the basic methods defined by `Module`.\n",
    "To fetch the parameters of the model, PyTorch can recursively call the `parameters()` for each submodule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most basic block is the linear layer, which defines an affine transformation of the input.\n",
    "\n",
    "`Linear` is characterized by:\n",
    "- `in_features` - the number of input features\n",
    "- `out_features` - the number of output features\n",
    "- `bias` - an optional boolean flag indicating whether to include a bias term\n",
    "\n",
    "$$\\text{output} = W\\cdot\\text{input} + b$$\n",
    "\n",
    "The linear layer is created as an **instance of the `torch.nn.Linear` class**.\n",
    "Thus, we first instantiate it, then we can use it as a function to transform input tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=10, out_features=1, bias=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin = torch.nn.Linear(\n",
    "    in_features=10,\n",
    "    out_features=1,\n",
    "    bias=True\n",
    ")\n",
    "\n",
    "lin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see its parameters by calling the `weight` and `bias` attributes, or by calling the `parameters()` method (or even better, the `named_parameters()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0626,  0.1555, -0.2141, -0.2305,  0.1476,  0.2546, -0.1666, -0.1842,\n",
      "         -0.0156, -0.2710]], requires_grad=True) \n",
      " Parameter containing:\n",
      "tensor([0.1635], requires_grad=True)\n",
      "------------------\n",
      "weight : torch.Size([1, 10])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0626,  0.1555, -0.2141, -0.2305,  0.1476,  0.2546, -0.1666, -0.1842,\n",
      "         -0.0156, -0.2710]], requires_grad=True)\n",
      "\n",
      "\n",
      "bias : torch.Size([1])\n",
      "Parameter containing:\n",
      "tensor([0.1635], requires_grad=True)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(lin.weight, \"\\n\", lin.bias)\n",
    "\n",
    "print(\"------------------\")\n",
    "for name, param in lin.named_parameters():\n",
    "    print(name, \":\", param.size())\n",
    "    print(param)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Take note of the `requires_grad` attribute of the parameters.\n",
    "We'll discuss it later.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000, 0.9000,\n",
      "        1.0000])\n",
      "------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.2780], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.linspace(0.1,1,10)\n",
    "print(x)\n",
    "print(\"------------------\")\n",
    "y = lin(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Take note of the `grad_fn=<AddBackward0>` attribute of the output tensor.\n",
    "We'll discuss it later.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**\n",
    "\n",
    "What is the Python pattern that allows us to call an object as a function?\n",
    "\n",
    "```python\n",
    "o = MyObject(...)\n",
    "o(x) # calling an instance of MyObject as a function\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For supporting the basic operations of the layer, PyTorch provides a set of functions in the `torch.nn.functional` module.\n",
    "\n",
    "The affine transformation is implemented as the `torch.nn.functional.linear` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2780], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.functional.linear(x, lin.weight, lin.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**\n",
    "\n",
    "Why don't we use the `torch.nn.functional.linear` function directly instead of defining a layer and calling it as a function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic differentiation\n",
    "\n",
    "The two weird names we saw before are related to the way Python keeps track of the operations performed on the tensors. \n",
    "Each time we perform an operation on a tensor, PyTorch keeps track of the operation in a **computational graph**.\n",
    "\n",
    "![](img/compgraph.png)\n",
    "\n",
    "The `grad_fn=<AddBackward0>` we observed on `y` makes explicit reference to the last operation performed to obtain `y` itself.\n",
    "\n",
    "Under the hood, PyTorch defines the tools to compute the backward pass on this function, i.e., the derivative of the summation.\n",
    "\n",
    "**Notice: unless you want to define exotic new computations, you'll never need to define the backward behavior of a function: PyTorch does everything by itself by combining the various building blocks it's provided with**\n",
    "\n",
    "We can prompt PyTorch to compute the gradients for each of the tensors involved in the computation by calling `backward()` on `y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now probe the gradients by calling `.grad` on the tensors involved in the computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000, 0.9000,\n",
      "         1.0000]])\n",
      "tensor([1.])\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(lin.weight.grad)\n",
    "\n",
    "print(lin.bias.grad)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**\n",
    "\n",
    "Why is `x.grad` `None`?\n",
    "\n",
    "The answer has to do with the second name, `requires_grad`.\n",
    "If a tensor does not require gradients, PyTorch will not compute them.\n",
    "\n",
    "We can force `x` to require gradients. Notice the change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0626,  0.1555, -0.2141, -0.2305,  0.1476,  0.2546, -0.1666, -0.1842,\n",
      "        -0.0156, -0.2710])\n"
     ]
    }
   ],
   "source": [
    "x.requires_grad = True\n",
    "y = lin(x)\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice another thing. Let's multiply two random tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2032,  0.2158, -1.4581,  0.2025],\n",
       "         [ 1.4985, -0.1969,  0.2244, -0.0836],\n",
       "         [-1.0066, -0.2862,  0.2189,  0.8253]],\n",
       "\n",
       "        [[ 0.0450, -0.5406,  1.4224,  0.5126],\n",
       "         [ 0.0625,  0.7091, -0.4182, -0.7371],\n",
       "         [ 0.5692,  0.9028,  0.2584, -0.0439]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.rand(2, 3, 4)\n",
    "t2 = torch.randn(2, 3, 4)\n",
    "t3 = t1 * t2\n",
    "t3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**\n",
    "\n",
    "Why does not `t3` have the `grad_fn` attribute?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our first neural network\n",
    "\n",
    "A regular Multilayer Perceptron (MLP) is defined as a sequence of linear layers with non-linearity in between.\n",
    "\n",
    "The classical way of building MLPs in PyTorch is by defining a class that inherits from `torch.nn.Module` and instantiates the layers in the constructor.\n",
    "\n",
    "Let's define a simple MLP with two hidden layers and a ReLU activation function.\n",
    "\n",
    "The two basic things to remember when you're defining a new `Module` are:\n",
    "\n",
    "* The constructor should call the constructor of the parent class (**this allows the instantiation of the important attribute and methods mentioned before**)\n",
    "* You should define the `forward` method, which describes how the input is transformed into the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyMLP(\n",
       "  (fc1): Linear(in_features=10, out_features=5, bias=True)\n",
       "  (batch_norm): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc2): Linear(in_features=5, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(10, 5)\n",
    "        self.batch_norm = nn.BatchNorm1d(5)\n",
    "        self.fc2 = nn.Linear(5, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "mlp = MyMLP()\n",
    "mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can already try to call this model on a random vector of size 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected 2D or 3D input (got 1D input)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[31], line 11\u001b[0m, in \u001b[0;36mMyMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x)\n\u001b[1;32m     10\u001b[0m x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[0;32m---> 11\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:138\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_input_dim\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# exponential_average_factor is set to self.momentum\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;66;03m# (when it is available) only so that it gets updated\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;66;03m# in ONNX graph when this node is exported to ONNX.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmomentum \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:301\u001b[0m, in \u001b[0;36mBatchNorm1d._check_input_dim\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_input_dim\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m--> 301\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    302\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected 2D or 3D input (got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124mD input)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim())\n\u001b[1;32m    303\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: expected 2D or 3D input (got 1D input)"
     ]
    }
   ],
   "source": [
    "mlp(torch.rand(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This error is because PyTorch already expects the input to come in batches.\n",
    "\n",
    "In case of MLP, the expected shape is $B \\times D$, where $B$ is the batch size and $D$ is the number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8375,  0.5648, -0.6774],\n",
      "        [ 0.3175,  0.4420, -0.0280],\n",
      "        [ 0.2310,  0.3613,  0.7035],\n",
      "        [ 0.3267,  0.1695,  0.1518]], grad_fn=<AddmmBackward0>) \n",
      " torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "out = mlp(torch.rand(4, 10)) # batch size of 4\n",
    "print(out, \"\\n\", out.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a model is composed of sequential layers and the information is not split into multiple branches, you can use the `torch.nn.Sequential` class, which allows you to define the model as a sequence of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4501,  0.4791, -0.1262],\n",
       "        [-0.1265,  0.2621, -0.1062],\n",
       "        [-0.1583, -0.3855, -0.3916],\n",
       "        [-0.1800, -1.2625, -0.5996]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = nn.Sequential(\n",
    "    nn.Linear(10, 5),\n",
    "    nn.ReLU(), # careful about the relu, we must pass `nn.Module`s here, not functions\n",
    "    nn.BatchNorm1d(5),\n",
    "    nn.Linear(5, 3)\n",
    ")\n",
    "\n",
    "out = mlp(torch.rand(4, 10))\n",
    "out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
